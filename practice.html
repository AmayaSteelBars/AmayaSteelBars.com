<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Object Detection with Bounding Boxes</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      text-align: center;
    }
    video, canvas {
      width: 80%;
      max-width: 500px;
      border: 1px solid #ddd;
    }
    #label {
      margin-top: 10px;
      font-size: 18px;
      color: green;
    }
    button {
      margin: 10px;
      padding: 10px 15px;
      font-size: 16px;
      cursor: pointer;
    }
    input {
      padding: 5px;
      font-size: 16px;
    }
  </style>
</head>
<body>
  <h1>Object Detection</h1>
  <video id="video" autoplay></video>
  <canvas id="canvas"></canvas>
  <p id="label">Detecting...</p>
  <button id="passButton">Pass to Search</button>
  <input type="text" id="searchBar" placeholder="Search here..." />
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>

  <script >
    
    const video = document.getElementById("video");
const canvas = document.getElementById("canvas");
const label = document.getElementById("label");
const searchBar = document.getElementById("searchBar");
const passButton = document.getElementById("passButton");

// Load the COCO-SSD model
let model;

// Load the Teachable Machine model
tf.loadGraphModel('model.json').then(loadedModel => {
  model = loadedModel;
  console.log("Custom model loaded!");
  startVideo(); // Start with the rear camera
}).catch(err => {
    console.error("Failed to load model:", err);
  });
// Function to start the camera feed
function startVideo(facingMode = "environment") {
  navigator.mediaDevices
    .getUserMedia({
      video: {
        facingMode: facingMode, // Rear camera: "environment", Front camera: "user"
      },
    })
    .then(stream => {
      video.srcObject = stream;
    })
    .catch(err => console.error("Error accessing the camera:", err));
}

// Detect objects and draw bounding boxes
video.addEventListener("loadeddata", () => {
  setInterval(() => {
    detectObjects();
  }, 500); // Run detection every 100ms
});

async function detectObjects() {
  if (!model) return;

  const context = canvas.getContext("2d");
  canvas.width = video.videoWidth;
  canvas.height = video.videoHeight;

  // Draw video frame to canvas
  context.drawImage(video, 0, 0, canvas.width, canvas.height);

  // Preprocess the video frame for the model
  const inputTensor = tf.browser.fromPixels(canvas)
    .resizeNearestNeighbor([224, 224]) // Resize to the input size expected by the model
    .toFloat()
    .expandDims(0)
    .div(tf.scalar(255)); // Normalize pixel values to [0, 1]

  // Make predictions
  const predictions = await model.predict(inputTensor).array();
  inputTensor.dispose(); // Free up memory

  // Process predictions
  const classIndex = predictions[0].indexOf(Math.max(...predictions[0])); // Find the class with the highest probability
  const confidence = predictions[0][classIndex]; // Get confidence score

  // Use metadata.json for class labels (if available)
  const labels = ["Class 1", "Class 2", "Class 3"]; // Replace with your actual class names
  const detectedLabel = labels[classIndex];

  // Display results
  if (confidence > 0.5) {
    label.textContent = `Detected: ${detectedLabel} (Confidence: ${(confidence * 100).toFixed(2)}%)`;

    // Pass the label to the search bar on button click
    passButton.onclick = () => {
      searchBar.value = detectedLabel;
    };
  } else {
    label.textContent = "No objects detected.";
  }
}

// Add a button to switch between front and rear cameras
const switchButton = document.createElement("button");
switchButton.textContent = "Switch Camera";
document.body.appendChild(switchButton);

let isFrontCamera = false;
switchButton.addEventListener("click", () => {
  isFrontCamera = !isFrontCamera;
  startVideo(isFrontCamera ? "user" : "environment");
});

  </script>
</body>
</html>